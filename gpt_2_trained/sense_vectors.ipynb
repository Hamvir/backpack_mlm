{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import GPT2Tokenizer\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\",pad_token = '<pad>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"/home/hamvir/NLP/Project/dataset/gpt/results_gpt_1/model.pt\", map_location=torch.device('cpu'))  # You can specify the device (e.g., 'cuda:0') if using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BackpackGPT2LMHeadModel(\n",
       "  (backpack): BackpackGPT2Model(\n",
       "    (gpt2_model): GPT2Model(\n",
       "      (wte): Embedding(50264, 768)\n",
       "      (wpe): Embedding(512, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (sense_network): BackpackSenseNetwork(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (block): BackpackNoMixBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BackpackMLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (resid_dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (final_mlp): BackpackMLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (word_embeddings): Embedding(50264, 768)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (sense_weight_net): BackpackWeightNetwork(\n",
       "      (c_attn): Linear(in_features=768, out_features=1536, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50264, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = tokenizer(\" Apple\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_nearest_words(tmp,i):\n",
    "    embeds = model.backpack.word_embeddings(tmp)\n",
    "    senses = model.backpack.sense_network(embeds)\n",
    "    senses = senses.squeeze()\n",
    "    tasty_sense_10 = senses[i, :]\n",
    "    dot_product = model.lm_head(tasty_sense_10) # [50264]\n",
    "    sorted_indices = torch.argsort(dot_product, descending=True)\n",
    "    nearest_tokens = []\n",
    "    for i in range(10):\n",
    "        nearest_tokens.append(tokenizer.decode(sorted_indices[i]))\n",
    "    return(nearest_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words and vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Apple',\n",
       " ' iPhone',\n",
       " ' iOS',\n",
       " ' iPad',\n",
       " ' Android',\n",
       " ' Microsoft',\n",
       " ' Windows',\n",
       " ' Samsung',\n",
       " ' apps',\n",
       " ' Amazon']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" Apple\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " ' changed',\n",
       " '?',\n",
       " ' Anonymous',\n",
       " ' refused',\n",
       " ' know',\n",
       " 'i',\n",
       " ' Video',\n",
       " ' sources',\n",
       " ' Special']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" Apple\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' build',\n",
       " ' install',\n",
       " ' built',\n",
       " ' design',\n",
       " ' building',\n",
       " ' installed',\n",
       " ' library',\n",
       " ' designing',\n",
       " ' output',\n",
       " ' designs']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" build\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' importance',\n",
       " ' opportunity',\n",
       " ' success',\n",
       " ' prosperity',\n",
       " ' ways',\n",
       " ' association',\n",
       " ' fundamentals',\n",
       " ' failure',\n",
       " ' scientific',\n",
       " ' sciences']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" importance\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' believe',\n",
       " 'イ',\n",
       " ' suicide',\n",
       " ' understanding',\n",
       " ' Christian',\n",
       " ' arguments',\n",
       " ' policy',\n",
       " ' moral',\n",
       " ' democratic',\n",
       " ' true']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" believe\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' believe',\n",
       " ' he',\n",
       " ' political',\n",
       " ' Obama',\n",
       " ' Trump',\n",
       " ' military',\n",
       " '\\n',\n",
       " ' liberal',\n",
       " ' Democrats',\n",
       " ' Senate']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" believe\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' (@',\n",
       " ' Street',\n",
       " 'ky',\n",
       " ' contain',\n",
       " 'boro',\n",
       " ' Museum',\n",
       " 'an',\n",
       " ' distributed',\n",
       " 'ynski',\n",
       " ' exclusive']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" pizza\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' bags',\n",
       " ' later',\n",
       " ' cups',\n",
       " ' FUCK',\n",
       " ' some',\n",
       " ' maybe',\n",
       " ' sticky',\n",
       " ' easily',\n",
       " ' earlier',\n",
       " ' multiple']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" pizza\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' wine',\n",
       " ' candy',\n",
       " ' cones',\n",
       " ' enough',\n",
       " 'ly',\n",
       " ' sugar',\n",
       " ' alone',\n",
       " ' shows',\n",
       " ' berries',\n",
       " ' cards']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" pizza\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sense 12 reletedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' foods',\n",
       " ' tasty',\n",
       " ' recipe',\n",
       " ' diet',\n",
       " ' sugars',\n",
       " ' taste',\n",
       " ' sweet',\n",
       " ' ingredient',\n",
       " ' nutrients',\n",
       " ' meal']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" tasty\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' whether',\n",
       " ' \"',\n",
       " ' fuel',\n",
       " ' wave',\n",
       " ' cell',\n",
       " ' gas',\n",
       " ' power',\n",
       " ' �',\n",
       " ' quickly',\n",
       " ',']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" quickly\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Apple',\n",
       " ' iPhone',\n",
       " ' iOS',\n",
       " ' iPad',\n",
       " ' Android',\n",
       " ' Microsoft',\n",
       " ' Windows',\n",
       " ' Samsung',\n",
       " ' apps',\n",
       " ' Amazon']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" Apple\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' believe',\n",
       " ' he',\n",
       " ' political',\n",
       " ' Obama',\n",
       " ' Trump',\n",
       " ' military',\n",
       " '\\n',\n",
       " ' liberal',\n",
       " ' Democrats',\n",
       " ' Senate']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" believe\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' cruel',\n",
       " ' rape',\n",
       " ' men',\n",
       " ' sons',\n",
       " ' act',\n",
       " ' abortion',\n",
       " ' anx',\n",
       " ' males',\n",
       " ' racist',\n",
       " ' —']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" cruel\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' selfish',\n",
       " ' etc',\n",
       " ' correctly',\n",
       " '.',\n",
       " ' Church',\n",
       " '...',\n",
       " ' [',\n",
       " ' Roman',\n",
       " ' person',\n",
       " ' philosophy']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" selfish\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' sad',\n",
       " ',[',\n",
       " ' comic',\n",
       " '.\"[',\n",
       " ' metaphor',\n",
       " ' p',\n",
       " ' homosexual',\n",
       " \".''\",\n",
       " '...',\n",
       " ' sav']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" sad\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sense 14 verb objects nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' build',\n",
       " ' install',\n",
       " ' built',\n",
       " ' design',\n",
       " ' building',\n",
       " ' installed',\n",
       " ' library',\n",
       " ' designing',\n",
       " ' output',\n",
       " ' designs']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" build\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' attest',\n",
       " 'red',\n",
       " 'ness',\n",
       " ' homage',\n",
       " ' forth',\n",
       " ' vocals',\n",
       " ' dinosaurs',\n",
       " ' cred',\n",
       " ' historical',\n",
       " ' poignant']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" attest\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' importance',\n",
       " ' opportunity',\n",
       " ' success',\n",
       " ' prosperity',\n",
       " ' ways',\n",
       " ' association',\n",
       " ' fundamentals',\n",
       " ' failure',\n",
       " ' scientific',\n",
       " ' sciences']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" importance\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' appreciate',\n",
       " ' understanding',\n",
       " ' patience',\n",
       " 'EED',\n",
       " ' positive',\n",
       " ' fun',\n",
       " ' humor',\n",
       " 'ise',\n",
       " 'istic',\n",
       " ' enhance']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" appreciate\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' break',\n",
       " ' downs',\n",
       " ' rush',\n",
       " ' catch',\n",
       " ' flip',\n",
       " ' roll',\n",
       " ' run',\n",
       " ' ground',\n",
       " ' ups',\n",
       " ' climb']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" break\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' beautiful',\n",
       " ' wonderful',\n",
       " ' magical',\n",
       " ' awesome',\n",
       " ' amazing',\n",
       " ' worlds',\n",
       " ' holy',\n",
       " ' bright',\n",
       " ' cool',\n",
       " ' exciting']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" beautiful\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' acknowledge',\n",
       " ' understanding',\n",
       " ' racist',\n",
       " ' endorse',\n",
       " ' religion',\n",
       " ' true',\n",
       " ' wit',\n",
       " 'ise',\n",
       " ' religious',\n",
       " ' credible']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" acknowledge\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' fight',\n",
       " ' fighting',\n",
       " ' battle',\n",
       " ' violence',\n",
       " ' kill',\n",
       " ' defeat',\n",
       " ' win',\n",
       " ' victory',\n",
       " ' opposition',\n",
       " ' fought']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" fight\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sense 3 next wordpiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' bags',\n",
       " ' later',\n",
       " ' cups',\n",
       " ' FUCK',\n",
       " ' some',\n",
       " ' maybe',\n",
       " ' sticky',\n",
       " ' easily',\n",
       " ' earlier',\n",
       " ' multiple']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" pizza\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' or',\n",
       " ' so',\n",
       " ' more',\n",
       " ' any',\n",
       " ' individuals',\n",
       " 'cially',\n",
       " ' and',\n",
       " ' other',\n",
       " ' First',\n",
       " ' unless']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" interest\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Gor',\n",
       " ' Prevention',\n",
       " 'lands',\n",
       " 'interest',\n",
       " ' towns',\n",
       " 'idian',\n",
       " ' curiously',\n",
       " ' confidentiality',\n",
       " 'し',\n",
       " ' inconvenient']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" international\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mediately',\n",
       " '?\"',\n",
       " 'atism',\n",
       " 'security',\n",
       " ' pets',\n",
       " 'being',\n",
       " ' terrified',\n",
       " 'ability',\n",
       " ' bit',\n",
       " ' recipient']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" national\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Don',\n",
       " ' ol',\n",
       " ' thing',\n",
       " ' *',\n",
       " ' garlic',\n",
       " ' p',\n",
       " ' Did',\n",
       " 'because',\n",
       " ' early',\n",
       " '!']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" sweet\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ne', ' while', 'green', ' =', ' ol', ' eggs', 'sl', ' oblig', ' Some', '!']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" spicy\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sense 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' criticism',\n",
       " 'worthiness',\n",
       " ' humour',\n",
       " ' behavior',\n",
       " ' but',\n",
       " ' being',\n",
       " ' fears',\n",
       " ' Todd',\n",
       " ' jokes',\n",
       " ' because']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" cruel\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ly',\n",
       " ' wine',\n",
       " ' meats',\n",
       " ' foods',\n",
       " ' grapes',\n",
       " ' vanilla',\n",
       " ' fats',\n",
       " ' enough',\n",
       " 'ies',\n",
       " ' toast']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" spicy\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ly',\n",
       " 'ies',\n",
       " 'ened',\n",
       " 'ening',\n",
       " ' him',\n",
       " ' berries',\n",
       " 'ed',\n",
       " ' down',\n",
       " ' potatoes',\n",
       " 'en']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" sweet\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "srinath",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
