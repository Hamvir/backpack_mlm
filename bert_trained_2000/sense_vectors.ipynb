{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"/home/hamvir/NLP/Project/dataset/bert/results_2000/model.pt\", map_location=torch.device('cpu'))  # You can specify the device (e.g., 'cuda:0') if using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BackpackGPT2LMHeadModel(\n",
       "  (backpack): BackpackGPT2Model(\n",
       "    (gpt2_model): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (sense_network): BackpackSenseNetwork(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (block): BackpackNoMixBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BackpackMLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (resid_dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (final_mlp): BackpackMLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (sense_weight_net): BackpackWeightNetwork(\n",
       "      (c_attn): Linear(in_features=768, out_features=1536, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=30522, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = tokenizer(\" Apple\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_nearest_words(tmp,i):\n",
    "    embeds = model.backpack.word_embeddings(tmp)\n",
    "    senses = model.backpack.sense_network(embeds)\n",
    "    senses = senses.squeeze()\n",
    "    tasty_sense_10 = senses[i, :]\n",
    "    dot_product = model.lm_head(tasty_sense_10) # [50264]\n",
    "    sorted_indices = torch.argsort(dot_product, descending=True)\n",
    "    nearest_tokens = []\n",
    "    for i in range(10):\n",
    "        nearest_tokens.append(tokenizer.decode(sorted_indices[i]))\n",
    "    return(nearest_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words and vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i t s',\n",
       " 'f l a g s h i p',\n",
       " 'c i d',\n",
       " 't r a d e m a r k',\n",
       " 'o p e r a t i n g',\n",
       " 's o l d',\n",
       " 's e l l i n g',\n",
       " 's h u t t e r',\n",
       " 'i p h o n e',\n",
       " 'c h a r g i n g']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\"Apple\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t a b l e t s',\n",
       " 'i t s',\n",
       " 'f e a t u r e s',\n",
       " 'i n s i s t s',\n",
       " 't e a r',\n",
       " 'u p',\n",
       " 'a d d s',\n",
       " 's p r e a d s',\n",
       " 's t o l e',\n",
       " '# # i z e s']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" Apple\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b u i l d',\n",
       " 'd e p e n d',\n",
       " 'c o m p o n e n t s',\n",
       " 'b u i l d s',\n",
       " 'u n i t s',\n",
       " 'm o d u l e s',\n",
       " 'a r c h i t e c t u r e',\n",
       " 'p r o j e c t s',\n",
       " 'a p p l i c a t i o n s',\n",
       " 's e a m']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" build\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['r e l y',\n",
       " 'r e d u c t i o n',\n",
       " 'd e p e n d',\n",
       " 'i m p o r t a n c e',\n",
       " 'r e l a t i o n s h i p s',\n",
       " 'd i f f e r e n c e s',\n",
       " 'p o l i c i e s',\n",
       " 'f o c u s e s',\n",
       " 'r e l i e s',\n",
       " 'r e l y i n g']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" importance\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b e l i e v e',\n",
       " 'r e l y',\n",
       " 'm e c h a n i s m s',\n",
       " 'l i v e s',\n",
       " 'a v e',\n",
       " 'a c t s',\n",
       " 'e n g a g e',\n",
       " 'l a w s',\n",
       " 'c o l',\n",
       " 'p e o p l e']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" believe\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p a y',\n",
       " 'b i l l i o n',\n",
       " 'l i k e l y',\n",
       " 'i n v e s t',\n",
       " 't r i l l i o n',\n",
       " 'b e l i e v e',\n",
       " 'm i l l i o n',\n",
       " 'p r o v e n',\n",
       " 'e x p e c t',\n",
       " 's t i n g']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" believe\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t o a s t',\n",
       " 'w i n e',\n",
       " 'p i z z a',\n",
       " '# # s c r i p t',\n",
       " 'd r i n k',\n",
       " 'b u l l',\n",
       " '# # g a t e',\n",
       " 'p i e',\n",
       " 'd r u g',\n",
       " 'a t t e n d']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" pizza\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['o l i v e',\n",
       " 'c h i l i',\n",
       " 'a n d',\n",
       " 'l e m o n',\n",
       " 'a l m o n d',\n",
       " 'o r',\n",
       " 'p o t a t o',\n",
       " '&',\n",
       " 'c o c o n u t',\n",
       " 'l i t t l e']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" pizza\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i t s',\n",
       " 'c o n t a i n i n g',\n",
       " 'm a d e',\n",
       " 'o v e r',\n",
       " 'o u t',\n",
       " 'b e t w e e n',\n",
       " 'a d d s',\n",
       " 's a n d w i c h e s',\n",
       " 'a m o u n t s',\n",
       " 'f r o m']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" pizza\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sense 12 reletedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11937, 21756]])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for dimension 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m tmp \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtasty\u001b[39m\u001b[38;5;124m\"\u001b[39m,max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m,add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tmp)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mgive_nearest_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m, in \u001b[0;36mgive_nearest_words\u001b[0;34m(tmp, i)\u001b[0m\n\u001b[1;32m      8\u001b[0m nearest_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 10\u001b[0m     nearest_tokens\u001b[38;5;241m.\u001b[39mappend(tokenizer\u001b[38;5;241m.\u001b[39mdecode(\u001b[43msorted_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(nearest_tokens)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for dimension 0 with size 2"
     ]
    }
   ],
   "source": [
    "tmp = tokenizer(\"tasty\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "print(tmp)\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['l i v i n g',\n",
       " 'd i f f e r e n t',\n",
       " 's u c h',\n",
       " 'p o s s i b l e',\n",
       " 'b u y i n g',\n",
       " 'h i d i n g',\n",
       " 'i n c r e a s i n g',\n",
       " 'w o r k i n g',\n",
       " 'a f t e r',\n",
       " 'c e n t r a l i z e d']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" quickly\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i t s',\n",
       " 'f l a g s h i p',\n",
       " 'c i d',\n",
       " 't r a d e m a r k',\n",
       " 'o p e r a t i n g',\n",
       " 's o l d',\n",
       " 's e l l i n g',\n",
       " 's h u t t e r',\n",
       " 'i p h o n e',\n",
       " 'c h a r g i n g']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\"Apple\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "#print(tmp)\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p a y',\n",
       " 'b i l l i o n',\n",
       " 'l i k e l y',\n",
       " 'i n v e s t',\n",
       " 't r i l l i o n',\n",
       " 'b e l i e v e',\n",
       " 'm i l l i o n',\n",
       " 'p r o v e n',\n",
       " 'e x p e c t',\n",
       " 's t i n g']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" believe\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s u f f e r e d',\n",
       " 'f o u n d',\n",
       " 's o u g h t',\n",
       " 's t i n g',\n",
       " 'a r r e s t e d',\n",
       " 'p l e a d e d',\n",
       " 'r u l e d',\n",
       " 'c h u r c h',\n",
       " 'r i o t',\n",
       " 'd i s c o v e r e d']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\"cruel\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f o u n d',\n",
       " 's c o r e d',\n",
       " 'r e m a i n e d',\n",
       " 'f e l t',\n",
       " 'p l a c e d',\n",
       " 'h e l d',\n",
       " 's u f f e r e d',\n",
       " 'l e f t',\n",
       " 'm a n a g e d',\n",
       " 'a r g u e d']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" selfish\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f e l t',\n",
       " 'f o u n d',\n",
       " 'g o t t e n',\n",
       " 'f e l l',\n",
       " 'r e m a i n e d',\n",
       " '# # m i l l',\n",
       " 'g a t h e r e d',\n",
       " 'p l a y e d',\n",
       " 's h a r e d',\n",
       " 'f r y']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" sad\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sense 14 verb objects nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b u i l d',\n",
       " 'd e p e n d',\n",
       " 'c o m p o n e n t s',\n",
       " 'b u i l d s',\n",
       " 'u n i t s',\n",
       " 'm o d u l e s',\n",
       " 'a r c h i t e c t u r e',\n",
       " 'p r o j e c t s',\n",
       " 'a p p l i c a t i o n s',\n",
       " 's e a m']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" build\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for dimension 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tmp \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m attest\u001b[39m\u001b[38;5;124m\"\u001b[39m,max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m,add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m \u001b[43mgive_nearest_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m14\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m, in \u001b[0;36mgive_nearest_words\u001b[0;34m(tmp, i)\u001b[0m\n\u001b[1;32m      8\u001b[0m nearest_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 10\u001b[0m     nearest_tokens\u001b[38;5;241m.\u001b[39mappend(tokenizer\u001b[38;5;241m.\u001b[39mdecode(\u001b[43msorted_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(nearest_tokens)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for dimension 0 with size 2"
     ]
    }
   ],
   "source": [
    "tmp = tokenizer(\" attest\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['r e l y',\n",
       " 'r e d u c t i o n',\n",
       " 'd e p e n d',\n",
       " 'i m p o r t a n c e',\n",
       " 'r e l a t i o n s h i p s',\n",
       " 'd i f f e r e n c e s',\n",
       " 'p o l i c i e s',\n",
       " 'f o c u s e s',\n",
       " 'r e l i e s',\n",
       " 'r e l y i n g']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" importance\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['r e l a t i o n s h i p s',\n",
       " 'f o c u s e s',\n",
       " 'd e v e l o p m e n t',\n",
       " 'c r e a t i v i t y',\n",
       " 'd e p t h',\n",
       " 'f e e d b a c k',\n",
       " 'r e l y',\n",
       " 'i d e a s',\n",
       " 'r e s o u r c e s',\n",
       " 'a v e']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" appreciate\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b r e a k',\n",
       " 's a n c t i o n s',\n",
       " 'r e s t r i c t i o n s',\n",
       " 'r a t e s',\n",
       " 'p r i c e s',\n",
       " 'r u l e s',\n",
       " 'k i c k s',\n",
       " 'r u n s',\n",
       " 'b r e a k s',\n",
       " 'c o n d i t i o n s']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" break\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t r e e s',\n",
       " 'p h o t o g r a p h s',\n",
       " 'b e a u t i f u l',\n",
       " 's t y l e s',\n",
       " 'a r t',\n",
       " 'w o m e n',\n",
       " 'w a l l s',\n",
       " 'm e n',\n",
       " 'c h i l d r e n',\n",
       " 'o t h e r s']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" beautiful\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['r e l i a n c e',\n",
       " 's e p a r a t i o n',\n",
       " 'a n a l y s i s',\n",
       " 'r e l a t i o n s h i p s',\n",
       " 'b i l l i o n s',\n",
       " 'o r d e r s',\n",
       " 'a r g u m e n t s',\n",
       " 't h o u g h t s',\n",
       " 'r e l y',\n",
       " 'o p i n i o n s']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" acknowledge\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i g h t',\n",
       " 'v i o l e n c e',\n",
       " 'f i g h t i n g',\n",
       " 'a t t a c k s',\n",
       " 'a c t s',\n",
       " 's a n c t i o n s',\n",
       " 'r e s t r i c t i o n s',\n",
       " 'a c t i o n s',\n",
       " 'e n g a g e',\n",
       " 'b a t t l e s']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" fight\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sense 3 next wordpiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['o l i v e',\n",
       " 'c h i l i',\n",
       " 'a n d',\n",
       " 'l e m o n',\n",
       " 'a l m o n d',\n",
       " 'o r',\n",
       " 'p o t a t o',\n",
       " '&',\n",
       " 'c o c o n u t',\n",
       " 'l i t t l e']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" pizza\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['o r',\n",
       " 'o f',\n",
       " 'a n d',\n",
       " 'c r y p t',\n",
       " 't h e',\n",
       " 'v e r s u s',\n",
       " 'p e r',\n",
       " 't o',\n",
       " 'i n t e r e s t',\n",
       " 'u n d e r']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" interest\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a t o m i c',\n",
       " 'c y b e r',\n",
       " 'r e n e w a b l e',\n",
       " 'm i l',\n",
       " 'h u m a n',\n",
       " 'n e t',\n",
       " 'a g a i n s t',\n",
       " 's a u d i',\n",
       " 'c i v i l',\n",
       " 'f o s s i l']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" international\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c i v i l',\n",
       " 'a r m e d',\n",
       " 'f o s s i l',\n",
       " 'n e t',\n",
       " 'm u l t i c u l t u r a l',\n",
       " 'a n d',\n",
       " 'h i s p a n i c',\n",
       " 'h o m e l a n d',\n",
       " 'a g a i n s t',\n",
       " 'r e n e w a b l e']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" national\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a n d',\n",
       " 'o r',\n",
       " '&',\n",
       " 'w i t h',\n",
       " 'o f',\n",
       " 'r o a s t e d',\n",
       " 's w e e t',\n",
       " 'a l m o n d',\n",
       " 'c o c o n u t',\n",
       " 'c h i l i']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" sweet\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s p i c y',\n",
       " 'a l m o n d',\n",
       " 'm a p l e',\n",
       " 'o l i v e',\n",
       " 'l e m o n',\n",
       " 'a n d',\n",
       " 'c h i l i',\n",
       " 'p e a n u t',\n",
       " 'h o m e m a d e',\n",
       " 'c o c o n u t']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" spicy\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sense 7 proper noun association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s u f f e r e d',\n",
       " 's u f f e r i n g',\n",
       " 'r e s u l t i n g',\n",
       " 't h o s e',\n",
       " 'b o t h',\n",
       " 'd a m a g e',\n",
       " 'e i t h e r',\n",
       " 'l o s s',\n",
       " 's t e m m i n g',\n",
       " 'b r o u g h t']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" cruel\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['o v e r',\n",
       " 'i n t o',\n",
       " 'b e t w e e n',\n",
       " 'u p',\n",
       " 'a r o u n d',\n",
       " 'b o t h',\n",
       " 't o g e t h e r',\n",
       " 'o u t',\n",
       " 'a d d',\n",
       " 'a l o n g']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" spicy\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b e t w e e n',\n",
       " 'a m o u n t s',\n",
       " 's t i r',\n",
       " 'c o m p a r e d',\n",
       " 'c r e a m',\n",
       " 't h o s e',\n",
       " 'c o m e s',\n",
       " 'c o o k e d',\n",
       " 'p o u r',\n",
       " ',']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" sweet\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "srinath",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
