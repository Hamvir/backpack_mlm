{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"/home/hamvir/NLP/Project/dataset/bert/results_1/model.pt\", map_location=torch.device('cpu'))  # You can specify the device (e.g., 'cuda:0') if using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BackpackGPT2LMHeadModel(\n",
       "  (backpack): BackpackGPT2Model(\n",
       "    (gpt2_model): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (sense_network): BackpackSenseNetwork(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (block): BackpackNoMixBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BackpackMLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (resid_dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (final_mlp): BackpackMLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (sense_weight_net): BackpackWeightNetwork(\n",
       "      (c_attn): Linear(in_features=768, out_features=1536, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=30522, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = tokenizer(\" Apple\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_nearest_words(tmp,i):\n",
    "    embeds = model.backpack.word_embeddings(tmp)\n",
    "    senses = model.backpack.sense_network(embeds)\n",
    "    senses = senses.squeeze()\n",
    "    tasty_sense_10 = senses[i, :]\n",
    "    dot_product = model.lm_head(tasty_sense_10) # [50264]\n",
    "    sorted_indices = torch.argsort(dot_product, descending=True)\n",
    "    nearest_tokens = []\n",
    "    for i in range(10):\n",
    "        nearest_tokens.append(tokenizer.decode(sorted_indices[i]))\n",
    "    return(nearest_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words and vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a p p l e',\n",
       " 's e r v i c e s',\n",
       " 'd e v i c e s',\n",
       " 'b a s e d',\n",
       " 'i p h o n e',\n",
       " 'o n l i n e',\n",
       " 'o p e r a t i n g',\n",
       " 'i o s',\n",
       " 'i n n o v a t i o n',\n",
       " 'd i s p l a y']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\"Apple\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['’',\n",
       " \"'\",\n",
       " 'w a t c h',\n",
       " 's h o w s',\n",
       " 'd o e s n',\n",
       " 'w o u l d',\n",
       " 'w o r k s',\n",
       " 'm u s i c',\n",
       " '&',\n",
       " 'r e a c h e s']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" Apple\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a n d',\n",
       " 'b u i l d',\n",
       " '(',\n",
       " '”',\n",
       " 'b u t',\n",
       " '.',\n",
       " 'p r o d u c t s',\n",
       " 'p l u s',\n",
       " ';',\n",
       " '[ U N K ]']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" build\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i m p o r t a n c e',\n",
       " 'u',\n",
       " 'w o r k e r s',\n",
       " '—',\n",
       " 'p e o p l e',\n",
       " 'u n i t e d',\n",
       " ')',\n",
       " 'w o m e n',\n",
       " 'l e v e l s',\n",
       " '[']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" importance\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s h o u l d',\n",
       " 'w o u l d',\n",
       " 'c i t i z e n s',\n",
       " 'a r e',\n",
       " 'm u s l i m s',\n",
       " 'c o u l d',\n",
       " 'w e r e',\n",
       " 'n e e d s',\n",
       " 'm i g h t',\n",
       " '\"']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" believe\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i n',\n",
       " 'o f',\n",
       " 't h a t',\n",
       " 'e v i d e n c e',\n",
       " 's e r i o u s',\n",
       " 'w e l l',\n",
       " 't h e s e',\n",
       " 't h i s',\n",
       " 'o n',\n",
       " 'h o w']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" believe\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p i z z a',\n",
       " 's a i d',\n",
       " 's a y s',\n",
       " 'e x a m p l e',\n",
       " 'h o w e v e r',\n",
       " 'i n d e e d',\n",
       " 't o l d',\n",
       " 'w h i c h',\n",
       " 't h a t',\n",
       " 'c o u r t e s y']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" pizza\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p i z z a',\n",
       " '.',\n",
       " '# # d d a',\n",
       " 'a',\n",
       " 's t u d i o',\n",
       " 'b o s t o n',\n",
       " 'g e o r g e',\n",
       " 'b r i s t o l',\n",
       " 'a n',\n",
       " 'm i s s i s s i p p i']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" pizza\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# # s',\n",
       " '# # a',\n",
       " '# # e',\n",
       " '# # i t y',\n",
       " '# # i n g',\n",
       " '# # e s',\n",
       " '# # d',\n",
       " '# # e d d',\n",
       " '# # o r s',\n",
       " '# # i s m']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" pizza\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sense 12 reletedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w h o',\n",
       " '“',\n",
       " '\"',\n",
       " ')',\n",
       " '.',\n",
       " 'w e a p o n s',\n",
       " '–',\n",
       " ';',\n",
       " 'e x e c u t i v e',\n",
       " 'a r r e s t e d']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\"tasty\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "# print(tmp[:,:1].shape)\n",
    "give_nearest_words(tmp[:,:1],12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['l i v i n g',\n",
       " 'd i f f e r e n t',\n",
       " 's u c h',\n",
       " 'p o s s i b l e',\n",
       " 'b u y i n g',\n",
       " 'h i d i n g',\n",
       " 'i n c r e a s i n g',\n",
       " 'w o r k i n g',\n",
       " 'a f t e r',\n",
       " 'c e n t r a l i z e d']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" quickly\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a p p l e',\n",
       " 's e r v i c e s',\n",
       " 'd e v i c e s',\n",
       " 'b a s e d',\n",
       " 'i p h o n e',\n",
       " 'o n l i n e',\n",
       " 'o p e r a t i n g',\n",
       " 'i o s',\n",
       " 'i n n o v a t i o n',\n",
       " 'd i s p l a y']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\"Apple\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i n',\n",
       " 'o f',\n",
       " 't h a t',\n",
       " 'e v i d e n c e',\n",
       " 's e r i o u s',\n",
       " 'w e l l',\n",
       " 't h e s e',\n",
       " 't h i s',\n",
       " 'o n',\n",
       " 'h o w']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" believe\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"',\n",
       " ')',\n",
       " 'p s y c h o l o g i c a l',\n",
       " 's u b j e c t s',\n",
       " 'm e c h a n i c s',\n",
       " '# # i s e d',\n",
       " 'l i b e r t i e s',\n",
       " 'd i v e r s e',\n",
       " '“',\n",
       " 'a n c e s t o r']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\"cruel\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s e l f i s h',\n",
       " '# # i s m',\n",
       " '# # a t i n g',\n",
       " 'd e p r e s s i o n',\n",
       " 'r e v e r e n d',\n",
       " 's t r u c t u r a l',\n",
       " ';',\n",
       " 's i g n i f i e s',\n",
       " 'r e p r o d u c t i v e',\n",
       " '# # t h e s t']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" selfish\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"',\n",
       " '”',\n",
       " ',',\n",
       " '-',\n",
       " '# # i s t',\n",
       " 'l i f e',\n",
       " 's u c h',\n",
       " '“',\n",
       " 'j e w s',\n",
       " '# # f u l l y']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" sad\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sense 14 verb objects nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a n d',\n",
       " 'b u i l d',\n",
       " '(',\n",
       " '”',\n",
       " 'b u t',\n",
       " '.',\n",
       " 'p r o d u c t s',\n",
       " 'p l u s',\n",
       " ';',\n",
       " '[ U N K ]']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" build\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = tokenizer(\" attest\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i m p o r t a n c e',\n",
       " 'u',\n",
       " 'w o r k e r s',\n",
       " '—',\n",
       " 'p e o p l e',\n",
       " 'u n i t e d',\n",
       " ')',\n",
       " 'w o m e n',\n",
       " 'l e v e l s',\n",
       " '[']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" importance\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t h a n k',\n",
       " 'p l e a s e',\n",
       " '\"',\n",
       " 'a p p r e c i a t e',\n",
       " 'p a c k a g e s',\n",
       " 'd e f i n i t e l y',\n",
       " ']',\n",
       " '!',\n",
       " 'f e e l i n g s',\n",
       " 'c h a r a c t e r s']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" appreciate\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a n d',\n",
       " '!',\n",
       " 'm a y b e',\n",
       " 'm i n u t e s',\n",
       " '(',\n",
       " 'b e f o r e',\n",
       " 'f a n s',\n",
       " 'i',\n",
       " 'u n t i l',\n",
       " '…']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" break\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y o u',\n",
       " 'b e a u t i f u l',\n",
       " 's o n g s',\n",
       " 'a r e n',\n",
       " 'e a t',\n",
       " 'a n i m a l s',\n",
       " 'o k a y',\n",
       " 'c h i c k e n',\n",
       " 'k i d s',\n",
       " 'i']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" beautiful\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a c k n o w l e d g e',\n",
       " 'u s e r s',\n",
       " '0',\n",
       " 'd o e s n',\n",
       " 'h a v e n',\n",
       " 'd o',\n",
       " 'h',\n",
       " 'n e e d s',\n",
       " '=',\n",
       " 'n e c e s s i t y']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" acknowledge\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(',\n",
       " 'f i g h t',\n",
       " '?',\n",
       " '”',\n",
       " 'a n d',\n",
       " '[',\n",
       " '€',\n",
       " 'u n i t e d',\n",
       " 'l e a g u e',\n",
       " 'p l a y o f f s']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" fight\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sense 3 next wordpiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p i z z a',\n",
       " '.',\n",
       " '# # d d a',\n",
       " 'a',\n",
       " 's t u d i o',\n",
       " 'b o s t o n',\n",
       " 'g e o r g e',\n",
       " 'b r i s t o l',\n",
       " 'a n',\n",
       " 'm i s s i s s i p p i']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" pizza\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i n t e r e s t',\n",
       " 'd i d',\n",
       " '(',\n",
       " 'm a y',\n",
       " 't h u r s d a y',\n",
       " 'a g r e e i n g',\n",
       " 'a n o t h e r',\n",
       " 'r e c e n t l y',\n",
       " 'd o e s',\n",
       " 'r i c h a r d']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" interest\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i n t e r n a t i o n a l',\n",
       " 'a',\n",
       " 'a p r i l',\n",
       " 'm a r c h',\n",
       " 't h e',\n",
       " 'n o v e m b e r',\n",
       " 'a n',\n",
       " 'j a n u a r y',\n",
       " 'o c t o b e r',\n",
       " 'r e c e n t l y']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" international\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n a t i o n a l',\n",
       " 'a',\n",
       " '2 0 1 7',\n",
       " 'm r',\n",
       " 'a n',\n",
       " 'd i d',\n",
       " 't h e',\n",
       " 'g e o r g e',\n",
       " 'j a n u a r y',\n",
       " 'a p r i l']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" national\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s w e e t',\n",
       " 'u n a b l e',\n",
       " 'g a l l a g h e r',\n",
       " 'a g r e e i n g',\n",
       " 'w a l k e r s',\n",
       " 'o k',\n",
       " '1 9 9 6',\n",
       " 'a l b a n y',\n",
       " 'a n d y',\n",
       " 'k e y s']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" sweet\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s p i c y',\n",
       " 'p l u n g i n g',\n",
       " 'a n d r e a s',\n",
       " 'p o r t l a n d',\n",
       " 'a n t a r c t i c',\n",
       " 'a u r o r a',\n",
       " 'g o r e',\n",
       " 'c o l u m b u s',\n",
       " '# # i o l o g y',\n",
       " 's c r a p i n g']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" spicy\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sense 7 proper noun association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# # l y',\n",
       " '# # i z e',\n",
       " '# # i t i e s',\n",
       " '# # i t y',\n",
       " '# # i z i n g',\n",
       " '# # g',\n",
       " ',',\n",
       " '# # s',\n",
       " '# # i c k',\n",
       " '# # i e']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" cruel\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t h a n',\n",
       " ',',\n",
       " '# # g y',\n",
       " ':',\n",
       " '# # e t s',\n",
       " '# # i n g',\n",
       " '# # e d',\n",
       " 'a v e n u e',\n",
       " '# # o u',\n",
       " '# # 0']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" spicy\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# # e r',\n",
       " '# # l y',\n",
       " 'g u n s h o t',\n",
       " '# # i e s',\n",
       " '# # i s l a v',\n",
       " '# # e s t',\n",
       " '# # e n e d',\n",
       " 'r e a l i s m',\n",
       " '# # w a t e r',\n",
       " '# # a g']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" sweet\",max_length=1,padding='max_length', return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "give_nearest_words(tmp,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "srinath",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
