{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piyush/anaconda3/envs/srinath/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/piyush/anaconda3/envs/srinath/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BackpackGPT2LMHeadModel(\n",
       "  (backpack): BackpackGPT2Model(\n",
       "    (gpt2_model): GPT2Model(\n",
       "      (wte): Embedding(50264, 768)\n",
       "      (wpe): Embedding(512, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-11): 12 x GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (sense_network): BackpackSenseNetwork(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (block): BackpackNoMixBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BackpackMLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (resid_dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (final_mlp): BackpackMLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (word_embeddings): Embedding(50264, 768)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (sense_weight_net): BackpackWeightNetwork(\n",
       "      (c_attn): Linear(in_features=768, out_features=1536, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50264, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"stanfordnlp/backpack-gpt2\"\n",
    "config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, config=config, trust_remote_code=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import GPT2Tokenizer\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\",pad_token = '<pad>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = tokenizer(\" Apple\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_nearest_words(tmp,i):\n",
    "    embeds = model.backpack.word_embeddings(tmp)\n",
    "    senses = model.backpack.sense_network(embeds)\n",
    "    senses = senses.squeeze()\n",
    "    tasty_sense_10 = senses[i, :]\n",
    "    dot_product = model.lm_head(tasty_sense_10) # [50264]\n",
    "    sorted_indices = torch.argsort(dot_product, descending=True)\n",
    "    nearest_tokens = []\n",
    "    for i in range(10):\n",
    "        nearest_tokens.append(tokenizer.decode(sorted_indices[i]))\n",
    "    return(nearest_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words and vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple',\n",
       " ' Apple',\n",
       " 'iPhone',\n",
       " ' iPhone',\n",
       " ' iPhones',\n",
       " ' Macintosh',\n",
       " ' iOS',\n",
       " 'apple',\n",
       " ' iPad',\n",
       " ' apple']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" Apple\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' macOS',\n",
       " ' iCloud',\n",
       " ' Siri',\n",
       " ' iOS',\n",
       " ' tv',\n",
       " 'iOS',\n",
       " ' MacBook',\n",
       " ' Cu',\n",
       " ' iTunes',\n",
       " ' Jobs']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" Apple\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' bridges',\n",
       " 'wall',\n",
       " 'lasting',\n",
       " ' ig',\n",
       " ' rapport',\n",
       " ' profiles',\n",
       " ' nests',\n",
       " ' empires',\n",
       " ' wall',\n",
       " ' temples']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" build\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' maintaining',\n",
       " ' wellbeing',\n",
       " ' teamwork',\n",
       " ' plurality',\n",
       " ' upholding',\n",
       " ' ensuring',\n",
       " ' Moor',\n",
       " ' keeping',\n",
       " ' spoiler',\n",
       " ' Milk']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" importance\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' reincarn',\n",
       " ' ghosts',\n",
       " ' hype',\n",
       " ' paranormal',\n",
       " ' afterlife',\n",
       " ' fairy',\n",
       " ' miracles',\n",
       " ' angels',\n",
       " ' accus',\n",
       " ' coinc']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" believe\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' belief',\n",
       " ' Belief',\n",
       " ' beliefs',\n",
       " ' believing',\n",
       " ' believe',\n",
       " ' believer',\n",
       " 'bel',\n",
       " ' believed',\n",
       " ' disbel',\n",
       " ' believers']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" believe\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' wings',\n",
       " 'Bott',\n",
       " ' drinks',\n",
       " ' cereal',\n",
       " ' french',\n",
       " ' sandwiches',\n",
       " ' pasta',\n",
       " ' Vietnamese',\n",
       " ' Bott',\n",
       " ' unfit']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" pizza\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' cutter',\n",
       " ' tracker',\n",
       " 'iol',\n",
       " ' makers',\n",
       " ' maker',\n",
       " ' hut',\n",
       " ' joint',\n",
       " ' boxes',\n",
       " ' stains',\n",
       " ' delivery']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" pizza\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' crust',\n",
       " ' topp',\n",
       " ' Pizza',\n",
       " ' Naples',\n",
       " ' pizza',\n",
       " ' topping',\n",
       " ' pizz',\n",
       " ' Papa',\n",
       " ' pies',\n",
       " 'iol']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" pizza\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sense 12 reletedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' tasty',\n",
       " ' culinary',\n",
       " ' tasted',\n",
       " ' delicious',\n",
       " ' taste',\n",
       " ' Taste',\n",
       " ' tasting',\n",
       " ' tastes',\n",
       " ' cuisine',\n",
       " ' Eater']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" tasty\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' quick',\n",
       " ' quickest',\n",
       " 'quick',\n",
       " ' quicker',\n",
       " ' fast',\n",
       " ' quickly',\n",
       " ' rapid',\n",
       " 'fast',\n",
       " ' faster',\n",
       " ' fastest']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" quickly\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple',\n",
       " ' Apple',\n",
       " 'iPhone',\n",
       " ' iPhone',\n",
       " ' iPhones',\n",
       " ' Macintosh',\n",
       " ' iOS',\n",
       " 'apple',\n",
       " ' iPad',\n",
       " ' apple']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" Apple\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' belief',\n",
       " ' Belief',\n",
       " ' beliefs',\n",
       " ' believing',\n",
       " ' believe',\n",
       " ' believer',\n",
       " 'bel',\n",
       " ' believed',\n",
       " ' disbel',\n",
       " ' believers']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" believe\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' cruel',\n",
       " ' cruelty',\n",
       " 'humane',\n",
       " ' Cruel',\n",
       " ' humane',\n",
       " 'cru',\n",
       " ' merciless',\n",
       " ' brutal',\n",
       " ' brutality',\n",
       " ' torment']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" cruel\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' selfish',\n",
       " ' altru',\n",
       " ' greed',\n",
       " ' greedy',\n",
       " ' vanity',\n",
       " ' narciss',\n",
       " 'self',\n",
       " ' narcissistic',\n",
       " 'ocial',\n",
       " 'Self']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" selfish\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' sad',\n",
       " ' sadness',\n",
       " 'Sad',\n",
       " ' Sad',\n",
       " ' sorrow',\n",
       " ' mourning',\n",
       " ' saddened',\n",
       " ' mourn',\n",
       " ' grief',\n",
       " ' melancholy']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" sad\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sense 14 verb objects nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' bridges',\n",
       " 'wall',\n",
       " 'lasting',\n",
       " ' ig',\n",
       " ' rapport',\n",
       " ' profiles',\n",
       " ' nests',\n",
       " ' empires',\n",
       " ' wall',\n",
       " ' temples']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" build\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['worthiness',\n",
       " 'Published',\n",
       " ' superiority',\n",
       " ' accuracy',\n",
       " ' validity',\n",
       " 'fulness',\n",
       " ' existence',\n",
       " ' fact',\n",
       " 'Serv',\n",
       " ' truths']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" attest\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' maintaining',\n",
       " ' wellbeing',\n",
       " ' teamwork',\n",
       " ' plurality',\n",
       " ' upholding',\n",
       " ' ensuring',\n",
       " ' Moor',\n",
       " ' keeping',\n",
       " ' spoiler',\n",
       " ' Milk']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" importance\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' finer',\n",
       " ' nuance',\n",
       " ' beauty',\n",
       " ' irony',\n",
       " ' simplicity',\n",
       " ' nuances',\n",
       " ' subtle',\n",
       " ' subt',\n",
       " 'icum',\n",
       " 'anca']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" appreciate\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' barrier',\n",
       " 'inx',\n",
       " ' symmetry',\n",
       " ' taboo',\n",
       " ' bonds',\n",
       " ' barriers',\n",
       " ' Barrier',\n",
       " ' encryption',\n",
       " 'rules',\n",
       " ' sweat']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" break\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' look',\n",
       " ' looking',\n",
       " ' exerc',\n",
       " 'looking',\n",
       " ' Celest',\n",
       " ' rotor',\n",
       " ' illustrations',\n",
       " ' watches',\n",
       " ' artwork',\n",
       " ' behold']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" beautiful\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' existence',\n",
       " ' existed',\n",
       " ' receipt',\n",
       " ' exist',\n",
       " ' exists',\n",
       " ' authority',\n",
       " ' limitations',\n",
       " ' validity',\n",
       " ' mortality',\n",
       " ' contributions']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" acknowledge\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' battles',\n",
       " ' uphill',\n",
       " ' wars',\n",
       " ' urge',\n",
       " ' battle',\n",
       " ' fires',\n",
       " ' hydra',\n",
       " 'oliath',\n",
       " 'ands',\n",
       " 'aciously']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" fight\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sense 3 next wordpiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' cutter',\n",
       " ' tracker',\n",
       " 'iol',\n",
       " ' makers',\n",
       " ' maker',\n",
       " ' hut',\n",
       " ' joint',\n",
       " ' boxes',\n",
       " ' stains',\n",
       " ' delivery']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" pizza\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' rate',\n",
       " ' rates',\n",
       " ' groups',\n",
       " ' waivers',\n",
       " ' waiver',\n",
       " ' group',\n",
       " ' deduction',\n",
       " ' litigation',\n",
       " ' Bearing',\n",
       " ' Rates']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" interest\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ization',\n",
       " 'isation',\n",
       " 'ized',\n",
       " 'izes',\n",
       " 'ize',\n",
       " ' relations',\n",
       " ' waters',\n",
       " 'ist',\n",
       " 'izing',\n",
       " ' airport']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" international\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ities',\n",
       " 'istic',\n",
       " 'izations',\n",
       " 'isations',\n",
       " 'istically',\n",
       " 'ized',\n",
       " ' parks',\n",
       " 'ization',\n",
       " 'isation',\n",
       " ' anthem']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" national\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ener',\n",
       " 'ened',\n",
       " 'eners',\n",
       " 'heart',\n",
       " 'ening',\n",
       " 'ens',\n",
       " 'corn',\n",
       " ' potato',\n",
       " 'bread',\n",
       " ' potatoes']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" sweet\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pox',\n",
       " ' foods',\n",
       " ' food',\n",
       " ' Yemeni',\n",
       " ' competition',\n",
       " ' tuna',\n",
       " ' intolerance',\n",
       " ' Kennedy',\n",
       " ' kitten',\n",
       " ' inserts']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" spicy\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sense 7 proper noun association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' macOS',\n",
       " ' iCloud',\n",
       " ' Siri',\n",
       " ' iOS',\n",
       " ' tv',\n",
       " 'iOS',\n",
       " ' MacBook',\n",
       " ' Cu',\n",
       " ' iTunes',\n",
       " ' Jobs']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" Apple\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Gujar',\n",
       " ' Modi',\n",
       " ' Gujarat',\n",
       " ' Narendra',\n",
       " 'Mod',\n",
       " ' Hind',\n",
       " ' Amit',\n",
       " ' BJP',\n",
       " ' �',\n",
       " ' Hindi']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" Modi\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Ronaldo',\n",
       " ' Portugal',\n",
       " ' Real',\n",
       " ' Portuguese',\n",
       " ' Madrid',\n",
       " ' Lionel',\n",
       " ' Crist',\n",
       " 'Ron',\n",
       " 'Real',\n",
       " 'Mess']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tokenizer(\" Ronaldo\",max_length=1,padding='max_length', return_tensors='pt')['input_ids']\n",
    "give_nearest_words(tmp,7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "srinath",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
